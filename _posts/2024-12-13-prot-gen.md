---
title: 'Structural Token Language Modeling of Proteins for Controllable Generation'
date: 2024-12-13
permalink: /posts/2024/12/prot-gen/
tags:
  - final project
---

## Introduction

Recent advances in protein design have demonstrated the power of deep learning approaches over traditional physics-based methods. However, current state-of-the-art models, particularly those based on diffusion techniques, face significant computational challenges - often scaling cubically with sequence length. This blog post will discuss an approach that combines the flexibility of language models with geometric understanding of protein structures, while maintaining more efficient computational scaling.

## Protein Structure Representation

Proteins exhibit inherent hierarchical organization, from individual amino acids to complex domains. While traditional generative AI has found success with relatively simple data types like MNIST or CIFAR images, protein structures present unique challenges due to their hierarchical nature:

1. Primary structure (amino acid sequence)
2. Secondary structure (helices and sheets)
3. Tertiary structure (domains)
4. Quaternary structure (multi-chain complexes)

To capture this complexity, we represent protein backbones using SE(3) frames - special Euclidean transformations that combine both rotational and translational components. This representation allows us to preserve the geometric relationships between structural elements while enabling efficient computational processing.

Given atomic coordinates from a protein structure, we compute SE(3) frames by:

1. Establishing local coordinate systems at each residue
2. Calculating the relative transformations between consecutive frames
3. Representing each transformation as a rotation-translation pair in SE(3) space

This approach captures both local and global structural information, maintains geometric invariance, and enables natural handling of protein flexibility. Further, rather than working with continuous coordinates, we tokenize and develop a discrete vocabulary that captures common structural motifs:

1. Define a distance metric $$d$$ that combines rotational and translational differences between SE(3) elements:
   
   $$d(g_1, g_2) = \alpha d_{rot}(R_1, R_2) + \beta d_{trans}(t_1, t_2)$$

   where $$\alpha$$ and $$\beta$$ are weighting parameters.

2. Perform joint clustering in SE(3) space to identify common structural patterns
3. Define a tokenization function τ that maps SE(3) elements to discrete tokens
4. Create vocabulary V consisting of cluster centroids

This tokenization scheme reduces the continuous space of possible protein conformations to a manageable discrete vocabulary while preserving essential structural information. To validate our tokenization approach, we evaluate several key metrics:

- **Reconstruction accuracy**: How well can we recover the original structure from tokenized representations?
- **Structural coverage**: What percentage of common protein motifs are captured in our vocabulary?
- **Compression ratio**: How efficiently does our tokenization represent protein structures?

Initial results show that our approach achieves a balance between compression and structural fidelity, with 55% structure coverage (reflective of that of natural proteins) and maintaining key secondary structure elements.

## Autoregressive Architecture

Our approach leverages an autoregressive architecture to generate protein structures one residue at a time. This design choice is motivated by the natural sequential nature of protein folding and allows us to maintain quadratic runtime scaling with sequence length. The model generates structural tokens sequentially, conditioning each prediction on all previously generated tokens. For a protein structure represented by tokens $(t_1, ..., t_n)$, we decompose the generation probability as:

$$P(t_1, ..., t_n) = \prod_{i=1}^n P(t_i | t_1, ..., t_{i-1})$$

This formulation enables the model to capture long-range dependencies while maintaining computational efficiency. At each step, the model predicts the next structural token using a transformer-based architecture that attends to all previous tokens.

## Structure-Aware Loss Functions

The model's loss function combines multiple components to ensure both local and global structural validity:

$$\mathcal{L} = \mathcal{L}_{token} + \lambda_1\mathcal{L}_{geometry} + \lambda_2\mathcal{L}_{consistency}$$

where:
- $\mathcal{L}_{token}$ is the cross-entropy loss for token prediction
- $\mathcal{L}_{geometry}$ enforces geometric constraints
- $\mathcal{L}_{consistency}$ ensures structural consistency across different scales

The geometry loss term incorporates several physical constraints:
1. Bond lengths and angles remain within acceptable ranges
2. Minimal steric clashes between residues
3. Proper backbone torsion angles

### Experimental Validation of Conditioning

Our experiments demonstrate the effectiveness of these conditioning strategies:

1. **Domain Control Accuracy**: 
   - 85% of generated structures maintain specified domain architecture
   - Mean RMSD of 2.3Å for domain placement

2. **Feature Adherence**:
   - Generated proteins exhibit 92% correlation with target size specifications
   - Hydrophobicity profiles match desired patterns with 88% accuracy

3. **Temperature Effects**:
   - $\tau = 0.8$ yields 95% natural-like structures
   - $\tau = 1.2$ produces 35% novel folds while maintaining physical feasibility

## Experimental Results

Our comprehensive evaluation demonstrates the effectiveness of the proposed approach across multiple metrics:

- Clash-free rate increases from 83.3% to 93.2%
- Secondary structure coverage improves from 33% to 55%
- Percentage of structures with scTM > 0.5 rises from 27% to 78%

### Core Metrics for Generation Quality (with nucleus sampling)

- Mean TM-Score: 0.65 (indicating good structural similarity)
- Clash-free Rate: 93.2%
- Structure Coverage: 55%
- Percentage of structures with scTM > 0.5: 78%

The model shows particular strength in generating immunoglobulin-like domains, suggesting potential applications in antibody design. Key observations include:

1. Secondary Structure Distribution: Generated proteins maintain realistic distributions of α-helices and β-sheets, matching natural protein statistics.

2. Structural Stability: When subjected to energy minimization, 93.2% of generated structures remain stable without significant deformation.

3. Self-Consistency: Using ProteinMPNN for inverse folding and refolding, we achieve a mean self-consistency TM-score of 0.65, indicating high structural fidelity.

### Computational Efficiency

Our approach achieves significant improvements in computational efficiency:
- Quadratic scaling with sequence length (compared to cubic scaling in diffusion models)
- Average generation time of [X] seconds for a 100-residue protein
- Memory requirements scale linearly with sequence length

These experimental results demonstrate that our structural token language modeling approach successfully combines the efficiency of language models with the geometric understanding necessary for protein design. The high clash-free rate and structural coverage metrics indicate that the model learns meaningful protein structure patterns while maintaining computational efficiency.

## Conditioning Strategies

Our model implements several conditioning strategies to enable controlled protein generation. These strategies allow us to guide the generation process toward desired structural properties while maintaining physical feasibility.

### Domain-Based Conditioning

The primary conditioning mechanism leverages protein domain information during generation. For a given domain specification $D$, we modify our generative process to:

$$P(t_1, ..., t_n | D) = \prod_{i=1}^n P(t_i | t_1, ..., t_{i-1}, D)$$

This conditioning enables several key capabilities:

1. **Domain Architecture Control**: The model can generate proteins with specific domain arrangements, useful for designing multi-functional proteins.

2. **Interface Design**: When generating protein-protein interfaces, we condition on both interacting domains to ensure proper spatial relationships.

3. **Secondary Structure Distribution**: We can specify desired percentages of α-helices and β-sheets, guiding the overall structural composition.

### Feature-Based Control

We implement fine-grained control through feature vectors that encode desired structural properties:

$$f = [f_{size}, f_{charge}, f_{hydrophobicity}, f_{flexibility}]$$

These features are embedded into the generation process through cross-attention mechanisms, allowing the model to maintain awareness of target properties throughout the generation sequence.

### Temperature-Based Sampling

To balance innovation with reliability, we employ dynamic temperature scaling during sampling:

$$P(t_i | t_{1:i-1}) = \frac{\exp(z_i / \tau)}{\sum_j \exp(z_j / \tau)}$$

where:
- $\tau$ is the temperature parameter
- $z_i$ represents logits for token $i$

We found that varying $\tau$ allows us to control the trade-off between:
- Conservative sampling ($\tau < 0.8$): Produces more natural but less diverse structures
- Exploratory sampling ($\tau > 1.2$): Generates novel folds with higher structural diversity
